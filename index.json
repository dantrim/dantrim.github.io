[{"categories":null,"content":"Hello This is a template. ","date":"2021-01-15","objectID":"/about/:0:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Curriculum Vitae ","date":"2021-01-15","objectID":"/cv/:0:0","tags":null,"title":"CV","uri":"/cv/"},{"categories":["foo"],"content":"This is the article description.","date":"2021-01-15","objectID":"/my-first-post/","tags":["test","test2"],"title":"First Post Testing Hugo","uri":"/my-first-post/"},{"categories":["foo"],"content":" This is a place for dummy text. ","date":"2021-01-15","objectID":"/my-first-post/:0:0","tags":["test","test2"],"title":"First Post Testing Hugo","uri":"/my-first-post/"},{"categories":["foo"],"content":"Top Level (Header 3) This is an unattributed quote This is a quote with an attribution. — Foo Bar 1 Header 4 Hello world Header 5 Header 6 ","date":"2021-01-15","objectID":"/my-first-post/:0:1","tags":["test","test2"],"title":"First Post Testing Hugo","uri":"/my-first-post/"},{"categories":["foo"],"content":"Some Code Examples VHDL library ieee; use ieee.std_logic_1164.all; use ieee.numeric_std.all; entity top is port ( clk : in std_logic; rst_n : in std_logic; segments : out std_logic_vector(6 downto 0); digit_sel : out std_logic ); end top; architecture rtl of top is begin segments \u003c= (others =\u003e '0'); end architecture; C++ This is what I get for C++, #include \u003ciostream\u003e int main(int argc, char* argv[]) { std::cout \u003c\u003c \"Hello, world!\" \u003c\u003c std::endl; } Python And for Python, import numpy as np def main() : print(\"Hello, world!\") if __name__ == \"__main__\" : main() Rust And everybody’s favorite, fn main(){println!(\"Hello, world!\");} ","date":"2021-01-15","objectID":"/my-first-post/:0:2","tags":["test","test2"],"title":"First Post Testing Hugo","uri":"/my-first-post/"},{"categories":["foo"],"content":"Mathematical Formula The information entropy, $ H $, over a discrete data is given as, $$ H = - \\sum_i p_i \\cdot log_2 (p_i), $$ where the $ p_i $ are the probabilities of codes/states, indexed by $ i $, in the data. When considering a continuous distribution, interpreted as a probability distribution function that satisfies $ \\int_{-\\inf}^{+\\inf} p(x) dx = 1$, the information entropy is given by, $$ H(x) = - \\int_{-\\inf}^{+\\inf} p(x) \\cdot log_2[p(x)] dx. $$ Foo Bar is Foo Baz ↩︎ ","date":"2021-01-15","objectID":"/my-first-post/:0:3","tags":["test","test2"],"title":"First Post Testing Hugo","uri":"/my-first-post/"}]